{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673a294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from starter.utils import get_device, get_mesh_renderer, load_cow_mesh, get_points_renderer, unproject_depth_image\n",
    "from starter.render_generic import load_rgbd_data\n",
    "import mcubes\n",
    "import pytorch3d\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "device = get_device()\n",
    "cow_path = \"data/cow.obj\"\n",
    "image_size=256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f114097",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e5d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gif_from_image_list(images_list: list[np.ndarray], gif_path: Path, FPS=15):\n",
    "    # images_list is a list of (H,W,3) images\n",
    "    assert images_list[0].shape[2] == 3\n",
    "    \n",
    "    frame_duration_ms = 1000 // FPS\n",
    "    imageio.mimsave(gif_path, images_list, duration=frame_duration_ms, loop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365f8146",
   "metadata": {},
   "source": [
    "## Question 1: Practicing with Cameras\n",
    "\n",
    "### 1.1: 360-deg renders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d970c8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering cow...: 100%|██████████| 36/36 [00:08<00:00,  4.08it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Get the renderer.\n",
    "renderer = get_mesh_renderer(image_size=image_size)\n",
    "\n",
    "# Get the vertices, faces, and textures.\n",
    "vertices, faces = load_cow_mesh(cow_path)\n",
    "vertices = vertices.unsqueeze(0)  # (N_v, 3) -> (1, N_v, 3)\n",
    "faces = faces.unsqueeze(0)  # (N_f, 3) -> (1, N_f, 3)\n",
    "textures = torch.ones_like(vertices)  # (1, N_v, 3)\n",
    "textures = textures * torch.tensor([0.7, 0.7, 1])  # (1, N_v, 3)\n",
    "\n",
    "mesh = pytorch3d.structures.Meshes(\n",
    "    verts=vertices,\n",
    "    faces=faces,\n",
    "    textures=pytorch3d.renderer.TexturesVertex(textures),\n",
    ")\n",
    "mesh = mesh.to(device)\n",
    "\n",
    "\n",
    "# Place a point light in front of the cow.\n",
    "lights = pytorch3d.renderer.PointLights(location=[[0, 0, -3]], device=device)\n",
    "\n",
    "images_list = []\n",
    "\n",
    "for i in tqdm(range(0,360,10), desc=\"Rendering cow...\"):\n",
    "\n",
    "    theta = np.radians(i)\n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    R = torch.tensor([[c, 0, s], [0, 1, 0], [-s, 0, c]]).unsqueeze(0)\n",
    "\n",
    "    # Prepare the camera:\n",
    "    cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "        R=R, T=torch.tensor([[0, 0, 3]]), fov=60, device=device\n",
    "    )\n",
    "    \n",
    "    rend = renderer(mesh, cameras=cameras, lights=lights)\n",
    "    img = rend.cpu().numpy()[0, ..., :3]\n",
    "        \n",
    "    img *= 255\n",
    "    img = img.astype('uint8')\n",
    "    images_list.append(img)\n",
    "\n",
    "create_gif_from_image_list(images_list, Path('out.gif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48f096e",
   "metadata": {},
   "source": [
    "## HW1Q1 result\n",
    "<img src=\"out.gif\" width=\"256\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d189bce",
   "metadata": {},
   "source": [
    "## 1.2: Dolly Zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19551fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.97it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_frames=10\n",
    "duration=3\n",
    "output_file=\"output/dolly.gif\"\n",
    "\n",
    "mesh = pytorch3d.io.load_objs_as_meshes([\"data/cow_on_plane.obj\"])\n",
    "mesh = mesh.to(device)\n",
    "renderer = get_mesh_renderer(image_size=image_size, device=device)\n",
    "lights = pytorch3d.renderer.PointLights(location=[[0.0, 0.0, -3.0]], device=device)\n",
    "\n",
    "fovs = torch.linspace(5, 120, num_frames)\n",
    "\n",
    "renders = []\n",
    "\n",
    "width = 5\n",
    "for fov in tqdm(fovs):\n",
    "    # distance = 50\n",
    "    distance = width / (2 * np.tan(0.5 * np.radians(fov))) # TODO: change this.\n",
    "    T = [[0, 0, distance]]  # TODO: Change this.\n",
    "    cameras = pytorch3d.renderer.FoVPerspectiveCameras(fov=fov, T=T, device=device)\n",
    "    rend = renderer(mesh, cameras=cameras, lights=lights)\n",
    "    rend = rend[0, ..., :3].cpu().numpy()  # (N, H, W, 3)\n",
    "    renders.append(rend)\n",
    "\n",
    "images = []\n",
    "for i, r in enumerate(renders):\n",
    "    image = Image.fromarray((r * 255).astype(np.uint8))\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    draw.text((20, 20), f\"fov: {fovs[i]:.2f}\", fill=(255, 0, 0))\n",
    "    images.append(np.array(image))\n",
    "\n",
    "create_gif_from_image_list(images, Path('hw1q1p2.gif'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc4addf",
   "metadata": {},
   "source": [
    "## q1.2 Dolly Zoom Result\n",
    "<img src=\"hw1q1p2.gif\" width=\"256\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb088e0a",
   "metadata": {},
   "source": [
    "# Question 2: Practicing with Meshes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f214414f",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deaff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_mesh_to_gif(gif_path:Path, desc: str, V: torch.tensor, F: torch.tensor):    \n",
    "    # Get the renderer.\n",
    "    renderer = get_mesh_renderer(image_size=image_size)\n",
    "\n",
    "    # Get the vertices, faces, and textures.\n",
    "    vertices = V.unsqueeze(0)  # (N_v, 3) -> (1, N_v, 3)\n",
    "    faces = F.unsqueeze(0)  # (N_f, 3) -> (1, N_f, 3)\n",
    "    textures = torch.ones_like(vertices)  # (1, N_v, 3)\n",
    "    textures = textures * torch.tensor([0.7, 0.7, 1])  # (1, N_v, 3)\n",
    "    mesh = pytorch3d.structures.Meshes(\n",
    "        verts=vertices,\n",
    "        faces=faces,\n",
    "        textures=pytorch3d.renderer.TexturesVertex(textures),\n",
    "    )\n",
    "    mesh = mesh.to(device)\n",
    "    \n",
    "    # Place a point light in front of the cow.\n",
    "    lights = pytorch3d.renderer.PointLights(location=[[0, 0, -3]], device=device)\n",
    "\n",
    "    images_list = []\n",
    "\n",
    "    for i in tqdm(range(0, 360, 10), desc=\"Rendering \" + desc + \"...\"):\n",
    "        theta = np.radians(i)\n",
    "        c, s = np.cos(theta), np.sin(theta)\n",
    "        R = torch.tensor([[c, 0, s], [0, 1, 0], [-s, 0, c]]).unsqueeze(0)\n",
    "\n",
    "        # Prepare the camera:\n",
    "        cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "            R=R, T=torch.tensor([[0, 0, 3]]), fov=60, device=device\n",
    "        )\n",
    "        \n",
    "        rend = renderer(mesh, cameras=cameras, lights=lights)\n",
    "        img = rend.cpu().numpy()[0, ..., :3]\n",
    "            \n",
    "        img *= 255\n",
    "        img = img.astype('uint8')\n",
    "        images_list.append(img)\n",
    "    \n",
    "    create_gif_from_image_list(images_list, gif_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e436f5",
   "metadata": {},
   "source": [
    "## Q2.1 Construct a Tetrahedron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c83048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering tetrahedron...: 100%|██████████| 36/36 [00:00<00:00, 100.15it/s]\n"
     ]
    }
   ],
   "source": [
    "V_tetra = torch.tensor([\n",
    "    [-0.5,-0.5,-0.5],\n",
    "    [0,1,0],\n",
    "    [0,0,1],\n",
    "    [1,0,0]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "F_tetra = torch.tensor([\n",
    "    [0,1,2],\n",
    "    [1,2,3],\n",
    "    [0,2,3],\n",
    "    [0,1,3]\n",
    "], dtype=torch.long)\n",
    "render_mesh_to_gif(\"hw1q2_tetrahedron.gif\", \"tetrahedron\", V=V_tetra, F=F_tetra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9796b1f5",
   "metadata": {},
   "source": [
    "## Q2.1 Tetrahedron\n",
    "<image src=\"hw1q2_tetrahedron.gif\" width=\"256\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839facd1",
   "metadata": {},
   "source": [
    "## Q2.2 Construct a Cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b7aa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering cube...: 100%|██████████| 36/36 [00:00<00:00, 94.80it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "V_cube = torch.tensor([\n",
    "    [-0.5,-0.5,-0.5],\n",
    "    [0.5,-0.5,-0.5],\n",
    "    [-0.5,0.5,-0.5],\n",
    "    [0.5,0.5,-0.5],\n",
    "    \n",
    "    [-0.5,-0.5,0.5],\n",
    "    [0.5,-0.5,0.5],\n",
    "    [-0.5,0.5,0.5],\n",
    "    [0.5,0.5,0.5]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "F_cube = torch.tensor([\n",
    "    [0,1,2],\n",
    "    [1,2,3],\n",
    "    \n",
    "    [4,5,6],\n",
    "    [5,6,7],\n",
    "    \n",
    "    [0,1,4],\n",
    "    [1,4,5],\n",
    "    \n",
    "    [2,3,6],\n",
    "    [3,6,7],\n",
    "    \n",
    "    [0,2,4],\n",
    "    [2,4,6],\n",
    "    \n",
    "    [1,3,5],\n",
    "    [3,5,7]\n",
    "    \n",
    "], dtype=torch.long)\n",
    "render_mesh_to_gif(\"hw1q2_cube.gif\", \"cube\", V=V_cube, F=F_cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b4f4de",
   "metadata": {},
   "source": [
    "## Q2.2 Cube\n",
    "<image src=\"hw1q2_cube.gif\" width=\"256\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80d5400",
   "metadata": {},
   "source": [
    "# Q3. Retexturing a mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab312bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering cow...: 100%|██████████| 36/36 [00:00<00:00, 162.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get the renderer.\n",
    "renderer = get_mesh_renderer(image_size=image_size)\n",
    "\n",
    "# Get the vertices, faces, and textures.\n",
    "vertices_cow, faces_cow = load_cow_mesh(cow_path)\n",
    "vertices = vertices_cow.unsqueeze(0)  # (N_v, 3) -> (1, N_v, 3)\n",
    "faces = faces_cow.unsqueeze(0)  # (N_f, 3) -> (1, N_f, 3)\n",
    "zs = vertices[0,:,2]\n",
    "z_max = torch.max(zs)\n",
    "z_min = torch.min(zs)\n",
    "color1 =  torch.tensor([[0,0,1]], dtype=torch.float32)\n",
    "color2 = torch.tensor([[1,0,0]], dtype=torch.float32)\n",
    "alphas = ((zs - z_min) / (z_max - z_min)).unsqueeze(1)\n",
    "textures = (alphas @ color2 + (1 - alphas) @ color1).unsqueeze(0) # (1, N_v, 3)\n",
    "\n",
    "mesh = pytorch3d.structures.Meshes(\n",
    "    verts=vertices,\n",
    "    faces=faces,\n",
    "    textures=pytorch3d.renderer.TexturesVertex(textures),\n",
    ")\n",
    "mesh = mesh.to(device)\n",
    "\n",
    "\n",
    "# Place a point light in front of the cow.\n",
    "lights = pytorch3d.renderer.PointLights(location=[[0, 0, -3]], device=device)\n",
    "\n",
    "images_list = []\n",
    "\n",
    "for i in tqdm(range(0,360,10), desc=\"Rendering cow...\"):\n",
    "\n",
    "    theta = np.radians(i)\n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    R = torch.tensor([[c, 0, s], [0, 1, 0], [-s, 0, c]]).unsqueeze(0)\n",
    "\n",
    "    # Prepare the camera:\n",
    "    cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "        R=R, T=torch.tensor([[0, 0, 3]]), fov=60, device=device\n",
    "    )\n",
    "    \n",
    "    rend = renderer(mesh, cameras=cameras, lights=lights)\n",
    "    img = rend.cpu().numpy()[0, ..., :3]\n",
    "        \n",
    "    img *= 255\n",
    "    img = img.astype('uint8')\n",
    "    images_list.append(img)\n",
    "\n",
    "create_gif_from_image_list(images_list, Path('hw1q3_color.gif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac40f47",
   "metadata": {},
   "source": [
    "## Q3 Results\n",
    "<image src=\"hw1q3_color.gif\" width=\"256\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112af2cd",
   "metadata": {},
   "source": [
    "# Q4. Camera Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6188aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_textured_cow(\n",
    "    cow_path=\"data/cow.obj\",\n",
    "    R_relative=[[1, 0, 0], [0, 1, 0], [0, 0, 1]],\n",
    "    T_relative=[0, 0, 0],\n",
    "):\n",
    "    meshes = pytorch3d.io.load_objs_as_meshes([cow_path]).to(device)\n",
    "    R_relative = torch.tensor(R_relative).float()\n",
    "    T_relative = torch.tensor(T_relative).float()\n",
    "    R = R_relative @ torch.tensor([[1.0, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "    T = R_relative @ torch.tensor([0.0, 0, 3]) + T_relative\n",
    "    renderer = get_mesh_renderer(image_size=256)\n",
    "    cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "        R=R.unsqueeze(0), T=T.unsqueeze(0), device=device,\n",
    "    )\n",
    "    lights = pytorch3d.renderer.PointLights(location=[[0, 0.0, -3.0]], device=device,)\n",
    "    rend = renderer(meshes, cameras=cameras, lights=lights)\n",
    "    return rend[0, ..., :3].cpu().numpy()\n",
    "\n",
    "Rs = [\n",
    "        [[1,0,0],[0,1,0],[0,0,1]], #identity\n",
    "        [[0,1,0],[-1,0,0],[0,0,1]],#RotZ_cw_90\n",
    "        [[1,0,0],[0,1,0],[0,0,1]],\n",
    "        [[1,0,0],[0,1,0],[0,0,1]], \n",
    "        [[0,0,1],[0,1,0],[-1,0,0]],#RotY_cw_90\n",
    "    ]\n",
    "    \n",
    "Ts = [\n",
    "    [0,0,0],\n",
    "    [0,0,0],\n",
    "    [0,0,2], #zoom out\n",
    "    [0.5,-0.5,0], #move bottom left\n",
    "    [-3,0,3], #reset cam after rotation and go to z=+3\n",
    "]\n",
    "\n",
    "jpg_names = [\n",
    "    \"identity\",\n",
    "    \"q1_rotz_cw_90\",\n",
    "    \"q2_zoom_out\",\n",
    "    \"q3_move_bottom_left\",\n",
    "    \"q4_roty_cw_90\",\n",
    "]\n",
    "\n",
    "for i in range(len(Rs)):\n",
    "    img = render_textured_cow(R_relative=Rs[i], T_relative=Ts[i])\n",
    "    plt.imsave(\"hw1q4_\"+jpg_names[i]+\".jpg\", img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef1d1e9",
   "metadata": {},
   "source": [
    "# TODO add more text here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da4eeb2",
   "metadata": {},
   "source": [
    "# Q5 Rendering Generic 3D Representations\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e412449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def render_pointcloud_to_gif(\n",
    "    V: torch.Tensor,\n",
    "    rgb: torch.Tensor,\n",
    "    gif_path: Path,\n",
    "    background_color=(1, 1, 1),\n",
    "    downsample_factor=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Renders a point cloud.\n",
    "    \"\"\"\n",
    "    renderer = get_points_renderer(\n",
    "        image_size=image_size, background_color=background_color\n",
    "    )\n",
    "    \n",
    "    verts = V[::downsample_factor].to(device).unsqueeze(0)\n",
    "    rgb = rgb[::downsample_factor].to(device).unsqueeze(0)\n",
    "    point_cloud = pytorch3d.structures.Pointclouds(points=verts, features=rgb)\n",
    "    \n",
    "    image_list = []\n",
    "    for azimuth in tqdm(range(0, 360, 10), desc=\"Rendering pointcloud...\"): \n",
    "        R, T = pytorch3d.renderer.look_at_view_transform(6, 10, azimuth)\n",
    "        cameras = pytorch3d.renderer.FoVPerspectiveCameras(R=R, T=T, device=device)\n",
    "        rend = renderer(point_cloud, cameras=cameras)\n",
    "        img = rend.cpu().numpy()[0, ..., :3]  # (B, H, W, 4) -> (H, W, 3)\n",
    "        img *= 255\n",
    "        img = img.astype('uint8')\n",
    "        image_list.append(img)\n",
    "        \n",
    "    create_gif_from_image_list(image_list, gif_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4287f28b",
   "metadata": {},
   "source": [
    "## Q5.1: Rendering PointCloud from RGBD Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ce648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomg/miniconda3/envs/l3d/lib/python3.10/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:4324.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Rendering pointcloud...: 100%|██████████| 36/36 [00:40<00:00,  1.13s/it]\n",
      "Rendering pointcloud...: 100%|██████████| 36/36 [00:40<00:00,  1.13s/it]\n",
      "Rendering pointcloud...:  81%|████████  | 29/36 [01:07<00:16,  2.33s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m render_pointcloud_to_gif(V\u001b[38;5;241m=\u001b[39mpc1_points, rgb\u001b[38;5;241m=\u001b[39mpc1_rgb, gif_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhw1q5p1_pc1.gif\u001b[39m\u001b[38;5;124m\"\u001b[39m, downsample_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     10\u001b[0m render_pointcloud_to_gif(V\u001b[38;5;241m=\u001b[39mpc2_points, rgb\u001b[38;5;241m=\u001b[39mpc2_rgb, gif_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhw1q5p1_pc2.gif\u001b[39m\u001b[38;5;124m\"\u001b[39m, downsample_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mrender_pointcloud_to_gif\u001b[49m\u001b[43m(\u001b[49m\u001b[43mV\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munion_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrgb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munion_rgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgif_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhw1q5p1_pc_both.gif\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownsample_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m, in \u001b[0;36mrender_pointcloud_to_gif\u001b[0;34m(V, rgb, gif_path, background_color, downsample_factor)\u001b[0m\n\u001b[1;32m     21\u001b[0m R, T \u001b[38;5;241m=\u001b[39m pytorch3d\u001b[38;5;241m.\u001b[39mrenderer\u001b[38;5;241m.\u001b[39mlook_at_view_transform(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m10\u001b[39m, azimuth)\n\u001b[1;32m     22\u001b[0m cameras \u001b[38;5;241m=\u001b[39m pytorch3d\u001b[38;5;241m.\u001b[39mrenderer\u001b[38;5;241m.\u001b[39mFoVPerspectiveCameras(R\u001b[38;5;241m=\u001b[39mR, T\u001b[38;5;241m=\u001b[39mT, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 23\u001b[0m rend \u001b[38;5;241m=\u001b[39m \u001b[43mrenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoint_cloud\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcameras\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcameras\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m img \u001b[38;5;241m=\u001b[39m rend\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m3\u001b[39m]  \u001b[38;5;66;03m# (B, H, W, 4) -> (H, W, 3)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m img \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/l3d/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/l3d/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/l3d/lib/python3.10/site-packages/pytorch3d/renderer/points/renderer.py:56\u001b[0m, in \u001b[0;36mPointsRenderer.forward\u001b[0;34m(self, point_clouds, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, point_clouds, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 56\u001b[0m     fragments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrasterizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoint_clouds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# Construct weights based on the distance of a point to the true point.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# However, this could be done differently: e.g. predicted as opposed\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# to a function of the weights.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrasterizer\u001b[38;5;241m.\u001b[39mraster_settings\u001b[38;5;241m.\u001b[39mradius\n",
      "File \u001b[0;32m~/miniconda3/envs/l3d/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/l3d/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/l3d/lib/python3.10/site-packages/pytorch3d/renderer/points/rasterizer.py:161\u001b[0m, in \u001b[0;36mPointsRasterizer.forward\u001b[0;34m(self, point_clouds, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m points_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(point_clouds, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    160\u001b[0m raster_settings \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraster_settings\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraster_settings)\n\u001b[0;32m--> 161\u001b[0m idx, zbuf, dists2 \u001b[38;5;241m=\u001b[39m \u001b[43mrasterize_points\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpoints_proj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraster_settings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mradius\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraster_settings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mradius\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpoints_per_pixel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraster_settings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoints_per_pixel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbin_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraster_settings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbin_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_points_per_bin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraster_settings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_points_per_bin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m PointFragments(idx\u001b[38;5;241m=\u001b[39midx, zbuf\u001b[38;5;241m=\u001b[39mzbuf, dists\u001b[38;5;241m=\u001b[39mdists2)\n",
      "File \u001b[0;32m~/miniconda3/envs/l3d/lib/python3.10/site-packages/pytorch3d/renderer/points/rasterize_points.py:134\u001b[0m, in \u001b[0;36mrasterize_points\u001b[0;34m(pointclouds, image_size, radius, points_per_pixel, bin_size, max_points_per_bin)\u001b[0m\n\u001b[1;32m    130\u001b[0m     max_points_per_bin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m10000\u001b[39m, pointclouds\u001b[38;5;241m.\u001b[39m_P \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Function.apply cannot take keyword args, so we handle defaults in this\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# wrapper and call apply with positional args only\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_RasterizePoints\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpoints_packed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloud_to_packed_first_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_points_per_cloud\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mradius\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpoints_per_pixel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbin_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_points_per_bin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/l3d/lib/python3.10/site-packages/torch/autograd/function.py:576\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    575\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    584\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/l3d/lib/python3.10/site-packages/pytorch3d/renderer/points/rasterize_points.py:214\u001b[0m, in \u001b[0;36m_RasterizePoints.forward\u001b[0;34m(ctx, points, cloud_to_packed_first_idx, num_points_per_cloud, image_size, radius, points_per_pixel, bin_size, max_points_per_bin)\u001b[0m\n\u001b[1;32m    203\u001b[0m args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    204\u001b[0m     points,\n\u001b[1;32m    205\u001b[0m     cloud_to_packed_first_idx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m     max_points_per_bin,\n\u001b[1;32m    212\u001b[0m )\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# pyre-fixme[16]: Module `pytorch3d` has no attribute `_C`.\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m idx, zbuf, dists \u001b[38;5;241m=\u001b[39m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrasterize_points\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(points, idx)\n\u001b[1;32m    216\u001b[0m ctx\u001b[38;5;241m.\u001b[39mmark_non_differentiable(idx)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = load_rgbd_data() #dict_keys(['rgb1', 'mask1', 'depth1', 'rgb2', 'mask2', 'depth2', 'cameras1', 'cameras2'])\n",
    "pc1_points, pc1_rgb = unproject_depth_image(image=torch.Tensor(data['rgb1']), mask=torch.Tensor(data['mask1']), depth=torch.Tensor(data['depth1']), camera=data['cameras1'])\n",
    "pc2_points, pc2_rgb = unproject_depth_image(image=torch.Tensor(data['rgb2']), mask=torch.Tensor(data['mask2']), depth=torch.Tensor(data['depth2']), camera=data['cameras2'])\n",
    "union_points = torch.vstack([pc1_points,pc2_points])\n",
    "union_rgb = torch.vstack([pc1_rgb, pc2_rgb])\n",
    "\n",
    "render_pointcloud_to_gif(V=pc1_points, rgb=pc1_rgb, gif_path=\"hw1q5p1_pc1.gif\", downsample_factor=10)\n",
    "render_pointcloud_to_gif(V=pc2_points, rgb=pc2_rgb, gif_path=\"hw1q5p1_pc2.gif\", downsample_factor=10)\n",
    "render_pointcloud_to_gif(V=union_points, rgb=union_rgb, gif_path=\"hw1q5p1_pc_both.gif\", downsample_factor=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8304cc",
   "metadata": {},
   "source": [
    "### Q5.1 First Image PC | Second Image PC | Union PC\n",
    "<image src=\"hw1q5p1_pc1.gif\" width=\"256\">\n",
    "<image src=\"hw1q5p1_pc2.gif\" width=\"256\">\n",
    "<image src=\"hw1q5p1_pc_both.gif\" width=\"256\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc1c54e",
   "metadata": {},
   "source": [
    "## Q5.2 Parametric Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5306c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering pointcloud...: 100%|██████████| 36/36 [00:00<00:00, 259.41it/s]\n",
      "/tmp/ipykernel_12044/1847027474.py:32: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  x = r * np.cos(v)\n",
      "/tmp/ipykernel_12044/1847027474.py:33: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  y = r * np.sin(v)\n",
      "Rendering pointcloud...: 100%|██████████| 36/36 [00:00<00:00, 269.21it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sample_torus_to_pointcloud(num_samples=200):\n",
    "    phi = torch.linspace(0, 2 * np.pi, num_samples)\n",
    "    theta = torch.linspace(0, np.pi, num_samples)\n",
    "    # Densely sample phi and theta on a grid\n",
    "    Phi, Theta = torch.meshgrid(phi, theta)\n",
    "    \n",
    "    R = 2\n",
    "    r = 0.5\n",
    "    x = (R + r * torch.sin(Theta)) * torch.cos(Phi)\n",
    "    y = (R + r * torch.sin(Theta)) * torch.sin(Phi)\n",
    "    z = r * torch.cos(Theta)\n",
    "\n",
    "    points = torch.stack((x.flatten(), y.flatten(), z.flatten()), dim=1)\n",
    "    color = (points - points.min()) / (points.max() - points.min())\n",
    "    \n",
    "    return points, color\n",
    "\n",
    "def sample_cyl_to_pointcloud(num_samples=200):\n",
    "    u = torch.linspace(0, np.pi, num_samples)\n",
    "    v = torch.linspace(0, 2*np.pi, num_samples)\n",
    "\n",
    "    u,v = torch.meshgrid(u, v)\n",
    "    r = 2\n",
    "    \n",
    "    x = r * np.cos(v)\n",
    "    y = r * np.sin(v)\n",
    "    z = u\n",
    "\n",
    "    points = torch.stack((x.flatten(), y.flatten(), z.flatten()), dim=1)\n",
    "    color = (points - points.min()) / (points.max() - points.min())\n",
    "    \n",
    "    return points, color\n",
    "\n",
    "torus_pts, torus_color = sample_torus_to_pointcloud(num_samples=200)\n",
    "\n",
    "render_pointcloud_to_gif(V=torus_pts, rgb=torus_color, gif_path=\"hw1q5p2_torus.gif\", downsample_factor=10)\n",
    "\n",
    "cyl_pts, cyl_color = sample_cyl_to_pointcloud(num_samples=200)\n",
    "\n",
    "render_pointcloud_to_gif(V=cyl_pts, rgb=cyl_color, gif_path=\"hw1q5p2_cyl.gif\", downsample_factor=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1c44e5",
   "metadata": {},
   "source": [
    "## Q5.2 Results: Torus | Cylinder\n",
    "<image src=\"hw1q5p2_torus.gif\" width=\"256\">\n",
    "<image src=\"hw1q5p2_cyl.gif\" width=\"256\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ade39",
   "metadata": {},
   "source": [
    "## Q5.3 Implicit Surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a63130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_mesh_to_gif(\n",
    "        mesh: pytorch3d.structures.Meshes,\n",
    "        gif_path: Path):\n",
    "    lights = pytorch3d.renderer.PointLights(location=[[0, 0.0, -4.0]], device=device)\n",
    "    renderer = get_mesh_renderer(image_size=image_size, device=device)\n",
    "\n",
    "    image_list = []\n",
    "    for azimuth in tqdm(range(0, 360, 10), desc=\"Rendering mesh...\"): \n",
    "        R, T = pytorch3d.renderer.look_at_view_transform(6, 10, azimuth)\n",
    "        cameras = pytorch3d.renderer.FoVPerspectiveCameras(R=R, T=T, device=device)\n",
    "        rend = renderer(mesh, cameras=cameras, lights=lights)\n",
    "        img = rend.cpu().numpy()[0, ..., :3].clip(0,1)  # (B, H, W, 4) -> (H, W, 3)\n",
    "        img *= 255\n",
    "        img = img.astype('uint8')\n",
    "        image_list.append(img)\n",
    "        \n",
    "    create_gif_from_image_list(image_list, gif_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f5ac3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering mesh...: 100%|██████████| 36/36 [00:00<00:00, 159.67it/s]\n",
      "Rendering mesh...: 100%|██████████| 36/36 [00:00<00:00, 171.45it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_torus_mesh(voxel_size=64):\n",
    "    min_value = -1.6\n",
    "    max_value = 1.6\n",
    "    R = 1\n",
    "    r = 0.5\n",
    "    X, Y, Z = torch.meshgrid([torch.linspace(min_value, max_value, voxel_size)] * 3)\n",
    "    voxels = (torch.sqrt(X**2 + Y**2) - R)**2 + Z**2 - r**2\n",
    "    vertices, faces = mcubes.marching_cubes(mcubes.smooth(voxels), isovalue=0)\n",
    "    vertices = torch.tensor(vertices).float()\n",
    "    faces = torch.tensor(faces.astype(int))\n",
    "    # Vertex coordinates are indexed by array position, so we need to\n",
    "    # renormalize the coordinate system.\n",
    "    vertices = (vertices / voxel_size) * (max_value - min_value) + min_value\n",
    "    textures = (vertices - vertices.min()) / (vertices.max() - vertices.min())\n",
    "    textures = pytorch3d.renderer.TexturesVertex(vertices.unsqueeze(0))\n",
    "\n",
    "    mesh = pytorch3d.structures.Meshes([vertices], [faces], textures=textures).to(\n",
    "        device\n",
    "    )\n",
    "    return mesh\n",
    "\n",
    "\n",
    "def create_whatever_mesh(voxel_size=64):\n",
    "    min_value = -5\n",
    "    max_value = 5\n",
    "    X, Y, Z = torch.meshgrid([torch.linspace(min_value, max_value, voxel_size)] * 3)\n",
    "    voxels = X**4 + Y**4 + Z**4 + 2 * X**2 * Y**2 + 2 * X**2 * Z**2 + 2 * Y**2 * Z**2 + 8*X*Y*Z - 8 * X**2 - 8 * Y**2 - 8 * Z**2 + 15\n",
    "    vertices, faces = mcubes.marching_cubes(mcubes.smooth(voxels), isovalue=0)\n",
    "    vertices = torch.tensor(vertices).float()\n",
    "    faces = torch.tensor(faces.astype(int))\n",
    "    # Vertex coordinates are indexed by array position, so we need to\n",
    "    # renormalize the coordinate system.\n",
    "    vertices = (vertices / voxel_size) * (max_value - min_value) + min_value\n",
    "    textures = (vertices - vertices.min()) / (vertices.max() - vertices.min())\n",
    "    textures = pytorch3d.renderer.TexturesVertex(vertices.unsqueeze(0))\n",
    "\n",
    "    mesh = pytorch3d.structures.Meshes([vertices], [faces], textures=textures).to(device)\n",
    "    return mesh\n",
    "\n",
    "mesh = create_torus_mesh()\n",
    "render_mesh_to_gif(mesh, \"hw1q5p3_torus.gif\")\n",
    "\n",
    "mesh2 = create_whatever_mesh()\n",
    "render_mesh_to_gif(mesh2, \"hw1q5p3_whatever.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e5613c",
   "metadata": {},
   "source": [
    "## Q5.3 Implicit Surfaces: Torus | Custom Object\n",
    "<image src=\"hw1q5p3_torus.gif\" width=\"256\">\n",
    "<image src=\"hw1q5p3_whatever.gif\" width=\"256\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bad8d9",
   "metadata": {},
   "source": [
    "# TODO may have to change custom object if they are iffy about it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d2fa4a",
   "metadata": {},
   "source": [
    "# Q6: Do Something Fun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e71137",
   "metadata": {},
   "source": [
    "# TODO have fun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd03fc6e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "l3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
