{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "673a294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from starter.utils import get_device, get_mesh_renderer, load_cow_mesh, get_points_renderer, unproject_depth_image\n",
    "from starter.render_generic import load_rgbd_data\n",
    "import mcubes\n",
    "import pytorch3d\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageDraw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f114097",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29e5d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gif(images_list: list[np.ndarray], gif_path: Path, FPS=15):\n",
    "    # images_list is a list of (H,W,3) images\n",
    "    assert images_list[0].shape[2] == 3\n",
    "    \n",
    "    frame_duration_ms = 1000 // FPS\n",
    "    imageio.mimsave(gif_path, images_list, duration=frame_duration_ms, loop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365f8146",
   "metadata": {},
   "source": [
    "## Question 1: Practicing with Cameras\n",
    "\n",
    "### 1.1: 360-deg renders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d970c8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering cow...: 100%|██████████| 36/36 [00:00<00:00, 70.04it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = get_device()\n",
    "cow_path = \"data/cow.obj\"\n",
    "image_size=256\n",
    "\n",
    "# Get the renderer.\n",
    "renderer = get_mesh_renderer(image_size=image_size)\n",
    "\n",
    "# Get the vertices, faces, and textures.\n",
    "vertices, faces = load_cow_mesh(cow_path)\n",
    "vertices = vertices.unsqueeze(0)  # (N_v, 3) -> (1, N_v, 3)\n",
    "faces = faces.unsqueeze(0)  # (N_f, 3) -> (1, N_f, 3)\n",
    "textures = torch.ones_like(vertices)  # (1, N_v, 3)\n",
    "textures = textures * torch.tensor([0.7, 0.7, 1])  # (1, N_v, 3)\n",
    "\n",
    "mesh = pytorch3d.structures.Meshes(\n",
    "    verts=vertices,\n",
    "    faces=faces,\n",
    "    textures=pytorch3d.renderer.TexturesVertex(textures),\n",
    ")\n",
    "mesh = mesh.to(device)\n",
    "\n",
    "\n",
    "# Place a point light in front of the cow.\n",
    "lights = pytorch3d.renderer.PointLights(location=[[0, 0, -3]], device=device)\n",
    "\n",
    "images_list = []\n",
    "\n",
    "for i in tqdm(range(0,360,10), desc=\"Rendering cow...\"):\n",
    "\n",
    "    theta = np.radians(i)\n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    R = torch.tensor([[c, 0, s], [0, 1, 0], [-s, 0, c]]).unsqueeze(0)\n",
    "\n",
    "    # Prepare the camera:\n",
    "    cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "        R=R, T=torch.tensor([[0, 0, 3]]), fov=60, device=device\n",
    "    )\n",
    "    \n",
    "    rend = renderer(mesh, cameras=cameras, lights=lights)\n",
    "    img = rend.cpu().numpy()[0, ..., :3]\n",
    "        \n",
    "    img *= 255\n",
    "    img = img.astype('uint8')\n",
    "    images_list.append(img)\n",
    "\n",
    "create_gif(images_list, Path('out.gif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48f096e",
   "metadata": {},
   "source": [
    "## HW1Q1 result\n",
    "<img src=\"out.gif\" width=\"256\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d189bce",
   "metadata": {},
   "source": [
    "## 1.2: Dolly Zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19551fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/tmp/ipykernel_12044/910618457.py:19: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  distance = width / (2 * np.tan(0.5 * np.radians(fov))) # TODO: change this.\n",
      "100%|██████████| 10/10 [00:00<00:00, 177.39it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_size=256\n",
    "num_frames=10\n",
    "duration=3\n",
    "output_file=\"output/dolly.gif\"\n",
    "device = get_device()\n",
    "\n",
    "mesh = pytorch3d.io.load_objs_as_meshes([\"data/cow_on_plane.obj\"])\n",
    "mesh = mesh.to(device)\n",
    "renderer = get_mesh_renderer(image_size=image_size, device=device)\n",
    "lights = pytorch3d.renderer.PointLights(location=[[0.0, 0.0, -3.0]], device=device)\n",
    "\n",
    "fovs = torch.linspace(5, 120, num_frames)\n",
    "\n",
    "renders = []\n",
    "\n",
    "width = 5\n",
    "for fov in tqdm(fovs):\n",
    "    # distance = 50\n",
    "    distance = width / (2 * np.tan(0.5 * np.radians(fov))) # TODO: change this.\n",
    "    T = [[0, 0, distance]]  # TODO: Change this.\n",
    "    cameras = pytorch3d.renderer.FoVPerspectiveCameras(fov=fov, T=T, device=device)\n",
    "    rend = renderer(mesh, cameras=cameras, lights=lights)\n",
    "    rend = rend[0, ..., :3].cpu().numpy()  # (N, H, W, 3)\n",
    "    renders.append(rend)\n",
    "\n",
    "images = []\n",
    "for i, r in enumerate(renders):\n",
    "    image = Image.fromarray((r * 255).astype(np.uint8))\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    draw.text((20, 20), f\"fov: {fovs[i]:.2f}\", fill=(255, 0, 0))\n",
    "    images.append(np.array(image))\n",
    "\n",
    "create_gif(images, Path('hw1q1p2.gif'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc4addf",
   "metadata": {},
   "source": [
    "## q1.2 Dolly Zoom Result\n",
    "<img src=\"hw1q1p2.gif\" width=\"256\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb088e0a",
   "metadata": {},
   "source": [
    "# Question 2: Practicing with Meshes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f214414f",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0deaff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_and_save_gif(gif_path:Path, desc: str, V: torch.tensor, F: torch.tensor):\n",
    "    device = get_device()\n",
    "    \n",
    "    image_size=256\n",
    "\n",
    "    # Get the renderer.\n",
    "    renderer = get_mesh_renderer(image_size=image_size)\n",
    "\n",
    "    # Get the vertices, faces, and textures.\n",
    "    vertices = V\n",
    "    faces = F\n",
    "    vertices = vertices.unsqueeze(0)  # (N_v, 3) -> (1, N_v, 3)\n",
    "    faces = faces.unsqueeze(0)  # (N_f, 3) -> (1, N_f, 3)\n",
    "    textures = torch.ones_like(vertices)  # (1, N_v, 3)\n",
    "    textures = textures * torch.tensor([0.7, 0.7, 1])  # (1, N_v, 3)\n",
    "    mesh = pytorch3d.structures.Meshes(\n",
    "        verts=vertices,\n",
    "        faces=faces,\n",
    "        textures=pytorch3d.renderer.TexturesVertex(textures),\n",
    "    )\n",
    "    mesh = mesh.to(device)\n",
    "    \n",
    "    \n",
    "    # Place a point light in front of the cow.\n",
    "    lights = pytorch3d.renderer.PointLights(location=[[0, 0, -3]], device=device)\n",
    "\n",
    "    images_list = []\n",
    "\n",
    "    for i in tqdm(range(0, 360, 10), desc=\"Rendering \" + desc + \"...\"):\n",
    "\n",
    "        theta = np.radians(i)\n",
    "        c, s = np.cos(theta), np.sin(theta)\n",
    "        R = torch.tensor([[c, 0, s], [0, 1, 0], [-s, 0, c]]).unsqueeze(0)\n",
    "\n",
    "        # Prepare the camera:\n",
    "        cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "            R=R, T=torch.tensor([[0, 0, 3]]), fov=60, device=device\n",
    "        )\n",
    "        \n",
    "        rend = renderer(mesh, cameras=cameras, lights=lights)\n",
    "        img = rend.cpu().numpy()[0, ..., :3]\n",
    "            \n",
    "        img *= 255\n",
    "        img = img.astype('uint8')\n",
    "        images_list.append(img)\n",
    "    \n",
    "    create_gif(images_list, gif_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e436f5",
   "metadata": {},
   "source": [
    "## Q2.1 Construct a Tetrahedron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29c83048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering tetrahedron...: 100%|██████████| 36/36 [00:00<00:00, 159.13it/s]\n"
     ]
    }
   ],
   "source": [
    "V_tetra = torch.tensor([\n",
    "    [-0.5,-0.5,-0.5],\n",
    "    [0,1,0],\n",
    "    [0,0,1],\n",
    "    [1,0,0]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "F_tetra = torch.tensor([\n",
    "    [0,1,2],\n",
    "    [1,2,3],\n",
    "    [0,2,3],\n",
    "    [0,1,3]\n",
    "], dtype=torch.long)\n",
    "render_and_save_gif(\"hw1q2_tetrahedron.gif\", \"tetrahedron\", V=V_tetra, F=F_tetra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9796b1f5",
   "metadata": {},
   "source": [
    "## Q2.1 Tetrahedron\n",
    "<image src=\"hw1q2_tetrahedron.gif\" width=\"256\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839facd1",
   "metadata": {},
   "source": [
    "## Q2.2 Construct a Cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4b7aa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering cube...: 100%|██████████| 36/36 [00:00<00:00, 164.18it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "V_cube = torch.tensor([\n",
    "    [-0.5,-0.5,-0.5],\n",
    "    [0.5,-0.5,-0.5],\n",
    "    [-0.5,0.5,-0.5],\n",
    "    [0.5,0.5,-0.5],\n",
    "    \n",
    "    [-0.5,-0.5,0.5],\n",
    "    [0.5,-0.5,0.5],\n",
    "    [-0.5,0.5,0.5],\n",
    "    [0.5,0.5,0.5]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "F_cube = torch.tensor([\n",
    "    [0,1,2],\n",
    "    [1,2,3],\n",
    "    \n",
    "    [4,5,6],\n",
    "    [5,6,7],\n",
    "    \n",
    "    [0,1,4],\n",
    "    [1,4,5],\n",
    "    \n",
    "    [2,3,6],\n",
    "    [3,6,7],\n",
    "    \n",
    "    [0,2,4],\n",
    "    [2,4,6],\n",
    "    \n",
    "    [1,3,5],\n",
    "    [3,5,7]\n",
    "    \n",
    "], dtype=torch.long)\n",
    "render_and_save_gif(\"hw1q2_cube.gif\", \"cube\", V=V_cube, F=F_cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b4f4de",
   "metadata": {},
   "source": [
    "## Q2.2 Cube\n",
    "<image src=\"hw1q2_cube.gif\" width=\"256\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80d5400",
   "metadata": {},
   "source": [
    "# Q3. Retexturing a mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab312bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering cow...: 100%|██████████| 36/36 [00:00<00:00, 162.10it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = get_device()\n",
    "cow_path = \"data/cow.obj\"\n",
    "image_size=256\n",
    "\n",
    "# Get the renderer.\n",
    "renderer = get_mesh_renderer(image_size=image_size)\n",
    "\n",
    "# Get the vertices, faces, and textures.\n",
    "vertices, faces = load_cow_mesh(cow_path)\n",
    "vertices = vertices.unsqueeze(0)  # (N_v, 3) -> (1, N_v, 3)\n",
    "faces = faces.unsqueeze(0)  # (N_f, 3) -> (1, N_f, 3)\n",
    "zs = vertices[0,:,2]\n",
    "z_max = torch.max(zs)\n",
    "z_min = torch.min(zs)\n",
    "color1 =  torch.tensor([[0,0,1]], dtype=torch.float32)\n",
    "color2 = torch.tensor([[1,0,0]], dtype=torch.float32)\n",
    "alphas = ((zs - z_min) / (z_max - z_min)).unsqueeze(1)\n",
    "textures = (alphas @ color2 + (1 - alphas) @ color1).unsqueeze(0) # (1, N_v, 3)\n",
    "\n",
    "mesh = pytorch3d.structures.Meshes(\n",
    "    verts=vertices,\n",
    "    faces=faces,\n",
    "    textures=pytorch3d.renderer.TexturesVertex(textures),\n",
    ")\n",
    "mesh = mesh.to(device)\n",
    "\n",
    "\n",
    "# Place a point light in front of the cow.\n",
    "lights = pytorch3d.renderer.PointLights(location=[[0, 0, -3]], device=device)\n",
    "\n",
    "images_list = []\n",
    "\n",
    "for i in tqdm(range(0,360,10), desc=\"Rendering cow...\"):\n",
    "\n",
    "    theta = np.radians(i)\n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    R = torch.tensor([[c, 0, s], [0, 1, 0], [-s, 0, c]]).unsqueeze(0)\n",
    "\n",
    "    # Prepare the camera:\n",
    "    cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "        R=R, T=torch.tensor([[0, 0, 3]]), fov=60, device=device\n",
    "    )\n",
    "    \n",
    "    rend = renderer(mesh, cameras=cameras, lights=lights)\n",
    "    img = rend.cpu().numpy()[0, ..., :3]\n",
    "        \n",
    "    img *= 255\n",
    "    img = img.astype('uint8')\n",
    "    images_list.append(img)\n",
    "\n",
    "create_gif(images_list, Path('hw1q3_color.gif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac40f47",
   "metadata": {},
   "source": [
    "## Q3 Results\n",
    "<image src=\"hw1q3_color.gif\" width=\"256\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112af2cd",
   "metadata": {},
   "source": [
    "# Q4. Camera Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6188aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_textured_cow(\n",
    "    cow_path=\"data/cow.obj\",\n",
    "    image_size=256,\n",
    "    R_relative=[[1, 0, 0], [0, 1, 0], [0, 0, 1]],\n",
    "    T_relative=[0, 0, 0],\n",
    "    device=None,\n",
    "):\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "    meshes = pytorch3d.io.load_objs_as_meshes([cow_path]).to(device)\n",
    "    R_relative = torch.tensor(R_relative).float()\n",
    "    T_relative = torch.tensor(T_relative).float()\n",
    "    R = R_relative @ torch.tensor([[1.0, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "    T = R_relative @ torch.tensor([0.0, 0, 3]) + T_relative\n",
    "    renderer = get_mesh_renderer(image_size=256)\n",
    "    cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "        R=R.unsqueeze(0), T=T.unsqueeze(0), device=device,\n",
    "    )\n",
    "    lights = pytorch3d.renderer.PointLights(location=[[0, 0.0, -3.0]], device=device,)\n",
    "    rend = renderer(meshes, cameras=cameras, lights=lights)\n",
    "    return rend[0, ..., :3].cpu().numpy()\n",
    "\n",
    "Rs = [\n",
    "        [[1,0,0],[0,1,0],[0,0,1]], #identity\n",
    "        [[0,1,0],[-1,0,0],[0,0,1]],#RotZ_cw_90\n",
    "        [[1,0,0],[0,1,0],[0,0,1]],\n",
    "        [[1,0,0],[0,1,0],[0,0,1]], \n",
    "        [[0,0,1],[0,1,0],[-1,0,0]],#RotY_cw_90\n",
    "    ]\n",
    "    \n",
    "Ts = [\n",
    "    [0,0,0],\n",
    "    [0,0,0],\n",
    "    [0,0,2], #zoom out\n",
    "    [0.5,-0.5,0], #move bottom left\n",
    "    [-3,0,3], #reset cam after rotation and go to z=+3\n",
    "]\n",
    "\n",
    "jpg_names = [\n",
    "    \"identity\",\n",
    "    \"q1_rotz_cw_90\",\n",
    "    \"q2_zoom_out\",\n",
    "    \"q3_move_bottom_left\",\n",
    "    \"q4_roty_cw_90\",\n",
    "]\n",
    "\n",
    "for i in range(len(Rs)):\n",
    "    img = render_textured_cow(R_relative=Rs[i], T_relative=Ts[i])\n",
    "    plt.imsave(\"hw1q4_\"+jpg_names[i]+\".jpg\", img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef1d1e9",
   "metadata": {},
   "source": [
    "# TODO add more text here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da4eeb2",
   "metadata": {},
   "source": [
    "# Q5 Rendering Generic 3D Representations\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e412449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def render_pointcloud_to_gif(\n",
    "    V: torch.Tensor,\n",
    "    rgb: torch.Tensor,\n",
    "    gif_path: Path,\n",
    "    image_size=256,\n",
    "    background_color=(1, 1, 1),\n",
    "    device=None,\n",
    "    downsample_factor=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Renders a point cloud.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "    renderer = get_points_renderer(\n",
    "        image_size=image_size, background_color=background_color\n",
    "    )\n",
    "    \n",
    "    verts = V[::downsample_factor].to(device).unsqueeze(0)\n",
    "    rgb = rgb[::downsample_factor].to(device).unsqueeze(0)\n",
    "    point_cloud = pytorch3d.structures.Pointclouds(points=verts, features=rgb)\n",
    "    \n",
    "    image_list = []\n",
    "    for azimuth in tqdm(range(0, 360, 10), desc=\"Rendering pointcloud...\"): \n",
    "        R, T = pytorch3d.renderer.look_at_view_transform(6, 10, azimuth)\n",
    "        cameras = pytorch3d.renderer.FoVPerspectiveCameras(R=R, T=T, device=device)\n",
    "        rend = renderer(point_cloud, cameras=cameras)\n",
    "        img = rend.cpu().numpy()[0, ..., :3]  # (B, H, W, 4) -> (H, W, 3)\n",
    "        img *= 255\n",
    "        img = img.astype('uint8')\n",
    "        image_list.append(img)\n",
    "        \n",
    "    create_gif(image_list, gif_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4287f28b",
   "metadata": {},
   "source": [
    "## Q5.1: Rendering PointCloud from RGBD Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b07ce648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomg/miniconda3/envs/learning3d/lib/python3.10/site-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Rendering pointcloud...: 100%|██████████| 36/36 [00:00<00:00, 209.02it/s]\n",
      "Rendering pointcloud...: 100%|██████████| 36/36 [00:00<00:00, 216.56it/s]\n",
      "Rendering pointcloud...: 100%|██████████| 36/36 [00:00<00:00, 208.73it/s]\n"
     ]
    }
   ],
   "source": [
    "data = load_rgbd_data() #dict_keys(['rgb1', 'mask1', 'depth1', 'rgb2', 'mask2', 'depth2', 'cameras1', 'cameras2'])\n",
    "pc1_points, pc1_rgb = unproject_depth_image(image=torch.Tensor(data['rgb1']), mask=torch.Tensor(data['mask1']), depth=torch.Tensor(data['depth1']), camera=data['cameras1'])\n",
    "pc2_points, pc2_rgb = unproject_depth_image(image=torch.Tensor(data['rgb2']), mask=torch.Tensor(data['mask2']), depth=torch.Tensor(data['depth2']), camera=data['cameras2'])\n",
    "# print(pc1_points.shape) #125035,3\n",
    "# print(pc1_rgb.shape)    #125035,4\n",
    "union_points = torch.vstack([pc1_points,pc2_points])\n",
    "union_rgb = torch.vstack([pc1_rgb, pc2_rgb])\n",
    "\n",
    "render_pointcloud_to_gif(V=pc1_points, rgb=pc1_rgb, gif_path=\"hw1q5p1_pc1.gif\", downsample_factor=10)\n",
    "render_pointcloud_to_gif(V=pc2_points, rgb=pc2_rgb, gif_path=\"hw1q5p1_pc2.gif\", downsample_factor=10)\n",
    "render_pointcloud_to_gif(V=union_points, rgb=union_rgb, gif_path=\"hw1q5p1_pc_both.gif\", downsample_factor=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8304cc",
   "metadata": {},
   "source": [
    "### Q5.1 First Image PC | Second Image PC | Union PC\n",
    "<image src=\"hw1q5p1_pc1.gif\" width=\"256\">\n",
    "<image src=\"hw1q5p1_pc2.gif\" width=\"256\">\n",
    "<image src=\"hw1q5p1_pc_both.gif\" width=\"256\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc1c54e",
   "metadata": {},
   "source": [
    "## Q5.2 Parametric Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5306c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering pointcloud...: 100%|██████████| 36/36 [00:00<00:00, 259.41it/s]\n",
      "/tmp/ipykernel_12044/1847027474.py:32: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  x = r * np.cos(v)\n",
      "/tmp/ipykernel_12044/1847027474.py:33: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  y = r * np.sin(v)\n",
      "Rendering pointcloud...: 100%|██████████| 36/36 [00:00<00:00, 269.21it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sample_torus_to_pointcloud(num_samples=200, device=None):\n",
    "    \n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "\n",
    "    phi = torch.linspace(0, 2 * np.pi, num_samples)\n",
    "    theta = torch.linspace(0, np.pi, num_samples)\n",
    "    # Densely sample phi and theta on a grid\n",
    "    Phi, Theta = torch.meshgrid(phi, theta)\n",
    "    \n",
    "    R = 2\n",
    "    r = 0.5\n",
    "    x = (R + r * torch.sin(Theta)) * torch.cos(Phi)\n",
    "    y = (R + r * torch.sin(Theta)) * torch.sin(Phi)\n",
    "    z = r * torch.cos(Theta)\n",
    "\n",
    "    points = torch.stack((x.flatten(), y.flatten(), z.flatten()), dim=1)\n",
    "    color = (points - points.min()) / (points.max() - points.min())\n",
    "    \n",
    "    return points, color\n",
    "\n",
    "def sample_cyl_to_pointcloud(num_samples=200, device=None):\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "\n",
    "    u = torch.linspace(0, np.pi, num_samples)\n",
    "    v = torch.linspace(0, 2*np.pi, num_samples)\n",
    "\n",
    "    u,v = torch.meshgrid(u, v)\n",
    "    r = 2\n",
    "    \n",
    "    x = r * np.cos(v)\n",
    "    y = r * np.sin(v)\n",
    "    z = u\n",
    "\n",
    "    points = torch.stack((x.flatten(), y.flatten(), z.flatten()), dim=1)\n",
    "    color = (points - points.min()) / (points.max() - points.min())\n",
    "    \n",
    "    return points, color\n",
    "\n",
    "torus_pts, torus_color = sample_torus_to_pointcloud(num_samples=200)\n",
    "\n",
    "render_pointcloud_to_gif(V=torus_pts, rgb=torus_color, gif_path=\"hw1q5p2_torus.gif\", downsample_factor=10)\n",
    "\n",
    "cyl_pts, cyl_color = sample_cyl_to_pointcloud(num_samples=200)\n",
    "\n",
    "render_pointcloud_to_gif(V=cyl_pts, rgb=cyl_color, gif_path=\"hw1q5p2_cyl.gif\", downsample_factor=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1c44e5",
   "metadata": {},
   "source": [
    "## Q5.2 Results: Torus | Cylinder\n",
    "<image src=\"hw1q5p2_torus.gif\" width=\"256\">\n",
    "<image src=\"hw1q5p2_cyl.gif\" width=\"256\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ade39",
   "metadata": {},
   "source": [
    "## Q5.3 Implicit Surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67a63130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_mesh_to_gif(\n",
    "        mesh: pytorch3d.structures.Meshes,\n",
    "        gif_path: Path,\n",
    "        image_size=256,\n",
    "        device=None):\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "\n",
    "    lights = pytorch3d.renderer.PointLights(location=[[0, 0.0, -4.0]], device=device,)\n",
    "    renderer = get_mesh_renderer(image_size=image_size, device=device)\n",
    "\n",
    "    image_list = []\n",
    "    for azimuth in tqdm(range(0, 360, 10), desc=\"Rendering mesh...\"): \n",
    "        R, T = pytorch3d.renderer.look_at_view_transform(6, 10, azimuth)\n",
    "        cameras = pytorch3d.renderer.FoVPerspectiveCameras(R=R, T=T, device=device)\n",
    "        rend = renderer(mesh, cameras=cameras, lights=lights)\n",
    "        img = rend.cpu().numpy()[0, ..., :3].clip(0,1)  # (B, H, W, 4) -> (H, W, 3)\n",
    "        img *= 255\n",
    "        img = img.astype('uint8')\n",
    "        image_list.append(img)\n",
    "        \n",
    "    create_gif(image_list, gif_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97f5ac3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering mesh...: 100%|██████████| 36/36 [00:00<00:00, 159.67it/s]\n",
      "Rendering mesh...: 100%|██████████| 36/36 [00:00<00:00, 171.45it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_torus_mesh(voxel_size=64, device=None):\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "    min_value = -1.6\n",
    "    max_value = 1.6\n",
    "    R = 1\n",
    "    r = 0.5\n",
    "    X, Y, Z = torch.meshgrid([torch.linspace(min_value, max_value, voxel_size)] * 3)\n",
    "    voxels = (torch.sqrt(X**2 + Y**2) - R)**2 + Z**2 - r**2\n",
    "    vertices, faces = mcubes.marching_cubes(mcubes.smooth(voxels), isovalue=0)\n",
    "    vertices = torch.tensor(vertices).float()\n",
    "    faces = torch.tensor(faces.astype(int))\n",
    "    # Vertex coordinates are indexed by array position, so we need to\n",
    "    # renormalize the coordinate system.\n",
    "    vertices = (vertices / voxel_size) * (max_value - min_value) + min_value\n",
    "    textures = (vertices - vertices.min()) / (vertices.max() - vertices.min())\n",
    "    textures = pytorch3d.renderer.TexturesVertex(vertices.unsqueeze(0))\n",
    "\n",
    "    mesh = pytorch3d.structures.Meshes([vertices], [faces], textures=textures).to(\n",
    "        device\n",
    "    )\n",
    "    return mesh\n",
    "\n",
    "\n",
    "def create_whatever_mesh(voxel_size=64, device=None):\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "    min_value = -5\n",
    "    max_value = 5\n",
    "    X, Y, Z = torch.meshgrid([torch.linspace(min_value, max_value, voxel_size)] * 3)\n",
    "    voxels = X**4 + Y**4 + Z**4 + 2 * X**2 * Y**2 + 2 * X**2 * Z**2 + 2 * Y**2 * Z**2 + 8*X*Y*Z - 8 * X**2 - 8 * Y**2 - 8 * Z**2 + 15\n",
    "    vertices, faces = mcubes.marching_cubes(mcubes.smooth(voxels), isovalue=0)\n",
    "    vertices = torch.tensor(vertices).float()\n",
    "    faces = torch.tensor(faces.astype(int))\n",
    "    # Vertex coordinates are indexed by array position, so we need to\n",
    "    # renormalize the coordinate system.\n",
    "    vertices = (vertices / voxel_size) * (max_value - min_value) + min_value\n",
    "    textures = (vertices - vertices.min()) / (vertices.max() - vertices.min())\n",
    "    textures = pytorch3d.renderer.TexturesVertex(vertices.unsqueeze(0))\n",
    "\n",
    "    mesh = pytorch3d.structures.Meshes([vertices], [faces], textures=textures).to(\n",
    "        device\n",
    "    )\n",
    "    return mesh\n",
    "\n",
    "mesh = create_torus_mesh()\n",
    "render_mesh_to_gif(mesh, \"hw1q5p3_torus.gif\")\n",
    "\n",
    "mesh2 = create_whatever_mesh()\n",
    "render_mesh_to_gif(mesh2, \"hw1q5p3_whatever.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e5613c",
   "metadata": {},
   "source": [
    "## Q5.3 Implicit Surfaces: Torus | Custom Object\n",
    "<image src=\"hw1q5p3_torus.gif\" width=\"256\">\n",
    "<image src=\"hw1q5p3_whatever.gif\" width=\"256\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bad8d9",
   "metadata": {},
   "source": [
    "# TODO may have to change custom object if they are iffy about it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d2fa4a",
   "metadata": {},
   "source": [
    "# Q6: Do Something Fun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e71137",
   "metadata": {},
   "source": [
    "# TODO have fun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd03fc6e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
