{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "673a294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from starter.utils import get_device, get_mesh_renderer, load_cow_mesh, get_points_renderer, unproject_depth_image\n",
    "from starter.render_generic import load_rgbd_data\n",
    "import mcubes\n",
    "import pytorch3d\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "device = get_device()\n",
    "cow_path = \"data/cow.obj\"\n",
    "image_size=256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f114097",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29e5d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gif_from_image_list(images_list: list[np.ndarray], gif_path: Path, FPS=15):\n",
    "    # images_list is a list of (H,W,3) images\n",
    "    assert images_list[0].shape[2] == 3\n",
    "    \n",
    "    frame_duration_ms = 1000 // FPS\n",
    "    imageio.mimsave(gif_path, images_list, duration=frame_duration_ms, loop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365f8146",
   "metadata": {},
   "source": [
    "## Question 1: Practicing with Cameras\n",
    "\n",
    "### 1.1: 360-deg renders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d970c8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering cow...: 100%|██████████| 36/36 [00:08<00:00,  4.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get the renderer.\n",
    "renderer = get_mesh_renderer(image_size=image_size)\n",
    "\n",
    "# Get the vertices, faces, and textures.\n",
    "vertices, faces = load_cow_mesh(cow_path)\n",
    "vertices = vertices.unsqueeze(0)  # (N_v, 3) -> (1, N_v, 3)\n",
    "faces = faces.unsqueeze(0)  # (N_f, 3) -> (1, N_f, 3)\n",
    "textures = torch.ones_like(vertices)  # (1, N_v, 3)\n",
    "textures = textures * torch.tensor([0.7, 0.7, 1])  # (1, N_v, 3)\n",
    "\n",
    "mesh = pytorch3d.structures.Meshes(\n",
    "    verts=vertices,\n",
    "    faces=faces,\n",
    "    textures=pytorch3d.renderer.TexturesVertex(textures),\n",
    ")\n",
    "mesh = mesh.to(device)\n",
    "\n",
    "\n",
    "# Place a point light in front of the cow.\n",
    "lights = pytorch3d.renderer.PointLights(location=[[0, 0, -3]], device=device)\n",
    "\n",
    "images_list = []\n",
    "\n",
    "for i in tqdm(range(0,360,10), desc=\"Rendering cow...\"):\n",
    "\n",
    "    theta = np.radians(i)\n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    R = torch.tensor([[c, 0, s], [0, 1, 0], [-s, 0, c]]).unsqueeze(0)\n",
    "\n",
    "    # Prepare the camera:\n",
    "    cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "        R=R, T=torch.tensor([[0, 0, 3]]), fov=60, device=device\n",
    "    )\n",
    "    \n",
    "    rend = renderer(mesh, cameras=cameras, lights=lights)\n",
    "    img = rend.cpu().numpy()[0, ..., :3]\n",
    "        \n",
    "    img *= 255\n",
    "    img = img.astype('uint8')\n",
    "    images_list.append(img)\n",
    "\n",
    "create_gif_from_image_list(images_list, Path('hw1q1p1.gif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48f096e",
   "metadata": {},
   "source": [
    "## HW1Q1 result\n",
    "<img src=\"hw1q1p1.gif\" width=\"256\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d189bce",
   "metadata": {},
   "source": [
    "## 1.2: Dolly Zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19551fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.93it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_frames=10\n",
    "duration=3\n",
    "output_file=\"output/dolly.gif\"\n",
    "\n",
    "mesh = pytorch3d.io.load_objs_as_meshes([\"data/cow_on_plane.obj\"])\n",
    "mesh = mesh.to(device)\n",
    "renderer = get_mesh_renderer(image_size=image_size, device=device)\n",
    "lights = pytorch3d.renderer.PointLights(location=[[0.0, 0.0, -3.0]], device=device)\n",
    "\n",
    "fovs = torch.linspace(5, 120, num_frames)\n",
    "\n",
    "renders = []\n",
    "\n",
    "width = 5\n",
    "for fov in tqdm(fovs):\n",
    "    # distance = 50\n",
    "    distance = width / (2 * np.tan(0.5 * np.radians(fov))) # TODO: change this.\n",
    "    T = [[0, 0, distance]]  # TODO: Change this.\n",
    "    cameras = pytorch3d.renderer.FoVPerspectiveCameras(fov=fov, T=T, device=device)\n",
    "    rend = renderer(mesh, cameras=cameras, lights=lights)\n",
    "    rend = rend[0, ..., :3].cpu().numpy()  # (N, H, W, 3)\n",
    "    renders.append(rend)\n",
    "\n",
    "images = []\n",
    "for i, r in enumerate(renders):\n",
    "    image = Image.fromarray((r * 255).astype(np.uint8))\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    draw.text((20, 20), f\"fov: {fovs[i]:.2f}\", fill=(255, 0, 0))\n",
    "    images.append(np.array(image))\n",
    "\n",
    "create_gif_from_image_list(images, Path('hw1q1p2.gif'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc4addf",
   "metadata": {},
   "source": [
    "## q1.2 Dolly Zoom Result\n",
    "<img src=\"hw1q1p2.gif\" width=\"256\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb088e0a",
   "metadata": {},
   "source": [
    "# Question 2: Practicing with Meshes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f214414f",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0deaff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_mesh_to_gif(gif_path:Path, desc: str, V: torch.tensor, F: torch.tensor):    \n",
    "    # Get the renderer.\n",
    "    renderer = get_mesh_renderer(image_size=image_size)\n",
    "\n",
    "    # Get the vertices, faces, and textures.\n",
    "    vertices = V.unsqueeze(0)  # (N_v, 3) -> (1, N_v, 3)\n",
    "    faces = F.unsqueeze(0)  # (N_f, 3) -> (1, N_f, 3)\n",
    "    textures = torch.ones_like(vertices)  # (1, N_v, 3)\n",
    "    textures = textures * torch.tensor([0.7, 0.7, 1])  # (1, N_v, 3)\n",
    "    mesh = pytorch3d.structures.Meshes(\n",
    "        verts=vertices,\n",
    "        faces=faces,\n",
    "        textures=pytorch3d.renderer.TexturesVertex(textures),\n",
    "    )\n",
    "    mesh = mesh.to(device)\n",
    "    \n",
    "    # Place a point light in front of the cow.\n",
    "    lights = pytorch3d.renderer.PointLights(location=[[0, 0, -3]], device=device)\n",
    "\n",
    "    images_list = []\n",
    "\n",
    "    for i in tqdm(range(0, 360, 10), desc=\"Rendering \" + desc + \"...\"):\n",
    "        theta = np.radians(i)\n",
    "        c, s = np.cos(theta), np.sin(theta)\n",
    "        R = torch.tensor([[c, 0, s], [0, 1, 0], [-s, 0, c]]).unsqueeze(0)\n",
    "\n",
    "        # Prepare the camera:\n",
    "        cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "            R=R, T=torch.tensor([[0, 0, 3]]), fov=60, device=device\n",
    "        )\n",
    "        \n",
    "        rend = renderer(mesh, cameras=cameras, lights=lights)\n",
    "        img = rend.cpu().numpy()[0, ..., :3]\n",
    "            \n",
    "        img *= 255\n",
    "        img = img.astype('uint8')\n",
    "        images_list.append(img)\n",
    "    \n",
    "    create_gif_from_image_list(images_list, gif_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e436f5",
   "metadata": {},
   "source": [
    "## Q2.1 Construct a Tetrahedron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29c83048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering tetrahedron...: 100%|██████████| 36/36 [00:00<00:00, 93.72it/s]\n"
     ]
    }
   ],
   "source": [
    "V_tetra = torch.tensor([\n",
    "    [-0.5,-0.5,-0.5],\n",
    "    [0,1,0],\n",
    "    [0,0,1],\n",
    "    [1,0,0]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "F_tetra = torch.tensor([\n",
    "    [0,1,2],\n",
    "    [1,2,3],\n",
    "    [0,2,3],\n",
    "    [0,1,3]\n",
    "], dtype=torch.long)\n",
    "render_mesh_to_gif(\"hw1q2_tetrahedron.gif\", \"tetrahedron\", V=V_tetra, F=F_tetra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9796b1f5",
   "metadata": {},
   "source": [
    "## Q2.1 Tetrahedron\n",
    "<image src=\"hw1q2_tetrahedron.gif\" width=\"256\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839facd1",
   "metadata": {},
   "source": [
    "## Q2.2 Construct a Cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4b7aa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering cube...: 100%|██████████| 36/36 [00:00<00:00, 97.01it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "V_cube = torch.tensor([\n",
    "    [-0.5,-0.5,-0.5],\n",
    "    [0.5,-0.5,-0.5],\n",
    "    [-0.5,0.5,-0.5],\n",
    "    [0.5,0.5,-0.5],\n",
    "    \n",
    "    [-0.5,-0.5,0.5],\n",
    "    [0.5,-0.5,0.5],\n",
    "    [-0.5,0.5,0.5],\n",
    "    [0.5,0.5,0.5]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "F_cube = torch.tensor([\n",
    "    [0,1,2],\n",
    "    [1,2,3],\n",
    "    \n",
    "    [4,5,6],\n",
    "    [5,6,7],\n",
    "    \n",
    "    [0,1,4],\n",
    "    [1,4,5],\n",
    "    \n",
    "    [2,3,6],\n",
    "    [3,6,7],\n",
    "    \n",
    "    [0,2,4],\n",
    "    [2,4,6],\n",
    "    \n",
    "    [1,3,5],\n",
    "    [3,5,7]\n",
    "    \n",
    "], dtype=torch.long)\n",
    "render_mesh_to_gif(\"hw1q2_cube.gif\", \"cube\", V=V_cube, F=F_cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b4f4de",
   "metadata": {},
   "source": [
    "## Q2.2 Cube\n",
    "<image src=\"hw1q2_cube.gif\" width=\"256\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80d5400",
   "metadata": {},
   "source": [
    "# Q3. Retexturing a mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab312bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering cow...: 100%|██████████| 36/36 [00:08<00:00,  4.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get the renderer.\n",
    "renderer = get_mesh_renderer(image_size=image_size)\n",
    "\n",
    "# Get the vertices, faces, and textures.\n",
    "vertices_cow, faces_cow = load_cow_mesh(cow_path)\n",
    "vertices = vertices_cow.unsqueeze(0)  # (N_v, 3) -> (1, N_v, 3)\n",
    "faces = faces_cow.unsqueeze(0)  # (N_f, 3) -> (1, N_f, 3)\n",
    "zs = vertices[0,:,2]\n",
    "z_max = torch.max(zs)\n",
    "z_min = torch.min(zs)\n",
    "color1 =  torch.tensor([[0,0,1]], dtype=torch.float32)\n",
    "color2 = torch.tensor([[1,0,0]], dtype=torch.float32)\n",
    "alphas = ((zs - z_min) / (z_max - z_min)).unsqueeze(1)\n",
    "textures = (alphas @ color2 + (1 - alphas) @ color1).unsqueeze(0) # (1, N_v, 3)\n",
    "\n",
    "mesh = pytorch3d.structures.Meshes(\n",
    "    verts=vertices,\n",
    "    faces=faces,\n",
    "    textures=pytorch3d.renderer.TexturesVertex(textures),\n",
    ")\n",
    "mesh = mesh.to(device)\n",
    "\n",
    "\n",
    "# Place a point light in front of the cow.\n",
    "lights = pytorch3d.renderer.PointLights(location=[[0, 0, -3]], device=device)\n",
    "\n",
    "images_list = []\n",
    "\n",
    "for i in tqdm(range(0,360,10), desc=\"Rendering cow...\"):\n",
    "\n",
    "    theta = np.radians(i)\n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    R = torch.tensor([[c, 0, s], [0, 1, 0], [-s, 0, c]]).unsqueeze(0)\n",
    "\n",
    "    # Prepare the camera:\n",
    "    cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "        R=R, T=torch.tensor([[0, 0, 3]]), fov=60, device=device\n",
    "    )\n",
    "    \n",
    "    rend = renderer(mesh, cameras=cameras, lights=lights)\n",
    "    img = rend.cpu().numpy()[0, ..., :3]\n",
    "        \n",
    "    img *= 255\n",
    "    img = img.astype('uint8')\n",
    "    images_list.append(img)\n",
    "\n",
    "create_gif_from_image_list(images_list, Path('hw1q3_color.gif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac40f47",
   "metadata": {},
   "source": [
    "## Q3 Results\n",
    "<image src=\"hw1q3_color.gif\" width=\"256\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112af2cd",
   "metadata": {},
   "source": [
    "# Q4. Camera Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6188aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_textured_cow(\n",
    "    cow_path=\"data/cow_with_axis.obj\",\n",
    "    R_relative=[[1, 0, 0], [0, 1, 0], [0, 0, 1]],\n",
    "    T_relative=[0, 0, 0],\n",
    "):\n",
    "    meshes = pytorch3d.io.load_objs_as_meshes([cow_path]).to(device)\n",
    "    R_relative = torch.tensor(R_relative).float()\n",
    "    T_relative = torch.tensor(T_relative).float()\n",
    "    R = R_relative @ torch.tensor([[1.0, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "    T = R_relative @ torch.tensor([0.0, 0, 3]) + T_relative\n",
    "    renderer = get_mesh_renderer(image_size=256)\n",
    "    cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "        R=R.unsqueeze(0), T=T.unsqueeze(0), device=device,\n",
    "    )\n",
    "    lights = pytorch3d.renderer.PointLights(location=[[0, 0.0, -3.0]], device=device,)\n",
    "    rend = renderer(meshes, cameras=cameras, lights=lights)\n",
    "        \n",
    "    return rend[0, ..., :3].cpu().numpy()\n",
    "\n",
    "Rs = [\n",
    "        [[1,0,0],[0,1,0],[0,0,1]], #identity\n",
    "        [[0,1,0],[-1,0,0],[0,0,1]],#transform 1: RotZ_cw_90\n",
    "        [[0,0,1],[0,1,0],[-1,0,0]],#transform 2: RotY_cw_90\n",
    "        [[1,0,0],[0,1,0],[0,0,1]],\n",
    "        [[1,0,0],[0,1,0],[0,0,1]], \n",
    "    ]\n",
    "    \n",
    "Ts = [\n",
    "    [0,0,0],\n",
    "    [0,0,0],\n",
    "    [-3,0,3],       #transform 2: reset cam after rotation and go to z=+3\n",
    "    [0,0,2],        #transform 3: zoom out\n",
    "    [0.5,-0.5,0],   #transform 4: move bottom left\n",
    "]\n",
    "\n",
    "jpg_names = [\n",
    "    \"identity\",\n",
    "    \"transform1\",\n",
    "    \"transform2\",\n",
    "    \"transform3\",\n",
    "    \"transform4\",\n",
    "]\n",
    "\n",
    "for i in range(len(Rs)):\n",
    "    img = render_textured_cow(R_relative=Rs[i], T_relative=Ts[i])\n",
    "    plt.imsave(\"hw1q4_\"+jpg_names[i]+\".jpg\", img)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef1d1e9",
   "metadata": {},
   "source": [
    "`R_relative` and `T_relative` transforms the camera view with respect to the default camera pose. For example, for transform 1, `R_relative` is used to rotate the view by the Z axis (pointing into the camera, out of the page) by 90 degrees clockwise; for transform 3, `T_relative` is used to move the camera 2 units along the Z axis such that the total distance from the cow is now 5 units instead of 3.\n",
    "\n",
    "\n",
    "## Q4 Rendered views: Identity | Transform 1 | Transform 2 | Transform 3 | Transform 4\n",
    "<img src=\"hw1q4_identity.jpg\" width=\"256\"/><img src=\"hw1q4_transform1.jpg\" width=\"256\"/><img src=\"hw1q4_transform2.jpg\" width=\"256\"/><img src=\"hw1q4_transform3.jpg\" width=\"256\"/><img src=\"hw1q4_transform4.jpg\" width=\"256\"/>\n",
    "\n",
    "Note that the transforms are named according to the views present in images/transform*.jpg, not in the order presented on the writeup (which shuffled the transform orders)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da4eeb2",
   "metadata": {},
   "source": [
    "# Q5 Rendering Generic 3D Representations\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e412449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def render_pointcloud_to_gif(\n",
    "    V: torch.Tensor,\n",
    "    rgb: torch.Tensor,\n",
    "    gif_path: Path,\n",
    "    background_color=(1, 1, 1),\n",
    "    downsample_factor=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Renders a point cloud.\n",
    "    \"\"\"\n",
    "    renderer = get_points_renderer(\n",
    "        image_size=image_size, background_color=background_color\n",
    "    )\n",
    "    \n",
    "    verts = V[::downsample_factor].to(device).unsqueeze(0)\n",
    "    rgb = rgb[::downsample_factor].to(device).unsqueeze(0)\n",
    "    point_cloud = pytorch3d.structures.Pointclouds(points=verts, features=rgb)\n",
    "    \n",
    "    image_list = []\n",
    "    for azimuth in tqdm(range(0, 360, 10), desc=\"Rendering pointcloud...\"): \n",
    "        R, T = pytorch3d.renderer.look_at_view_transform(6, 10, azimuth)\n",
    "        cameras = pytorch3d.renderer.FoVPerspectiveCameras(R=R, T=T, device=device)\n",
    "        rend = renderer(point_cloud, cameras=cameras)\n",
    "        img = rend.cpu().numpy()[0, ..., :3]  # (B, H, W, 4) -> (H, W, 3)\n",
    "        img *= 255\n",
    "        img = img.astype('uint8')\n",
    "        image_list.append(img)\n",
    "        \n",
    "    create_gif_from_image_list(image_list, gif_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4287f28b",
   "metadata": {},
   "source": [
    "## Q5.1: Rendering PointCloud from RGBD Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b07ce648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomg/miniconda3/envs/l3d/lib/python3.10/site-packages/torch/functional.py:554: UserWarning:\n",
      "\n",
      "torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:4324.)\n",
      "\n",
      "Rendering pointcloud...: 100%|██████████| 36/36 [00:39<00:00,  1.10s/it]\n",
      "Rendering pointcloud...: 100%|██████████| 36/36 [00:40<00:00,  1.12s/it]\n",
      "Rendering pointcloud...: 100%|██████████| 36/36 [01:21<00:00,  2.27s/it]\n"
     ]
    }
   ],
   "source": [
    "data = load_rgbd_data() #dict_keys(['rgb1', 'mask1', 'depth1', 'rgb2', 'mask2', 'depth2', 'cameras1', 'cameras2'])\n",
    "pc1_points, pc1_rgb = unproject_depth_image(image=torch.Tensor(data['rgb1']), mask=torch.Tensor(data['mask1']), depth=torch.Tensor(data['depth1']), camera=data['cameras1'])\n",
    "pc2_points, pc2_rgb = unproject_depth_image(image=torch.Tensor(data['rgb2']), mask=torch.Tensor(data['mask2']), depth=torch.Tensor(data['depth2']), camera=data['cameras2'])\n",
    "union_points = torch.vstack([pc1_points,pc2_points])\n",
    "union_rgb = torch.vstack([pc1_rgb, pc2_rgb])\n",
    "\n",
    "render_pointcloud_to_gif(V=pc1_points, rgb=pc1_rgb, gif_path=\"hw1q5p1_pc1.gif\", downsample_factor=10)\n",
    "render_pointcloud_to_gif(V=pc2_points, rgb=pc2_rgb, gif_path=\"hw1q5p1_pc2.gif\", downsample_factor=10)\n",
    "render_pointcloud_to_gif(V=union_points, rgb=union_rgb, gif_path=\"hw1q5p1_pc_both.gif\", downsample_factor=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8304cc",
   "metadata": {},
   "source": [
    "### Q5.1 First Image PC | Second Image PC | Union PC\n",
    "<image src=\"hw1q5p1_pc1.gif\" width=\"256\">\n",
    "<image src=\"hw1q5p1_pc2.gif\" width=\"256\">\n",
    "<image src=\"hw1q5p1_pc_both.gif\" width=\"256\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc1c54e",
   "metadata": {},
   "source": [
    "## Q5.2 Parametric Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5306c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering pointcloud...: 100%|██████████| 36/36 [03:48<00:00,  6.35s/it]\n",
      "Rendering pointcloud...: 100%|██████████| 36/36 [02:09<00:00,  3.61s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sample_torus_to_pointcloud(num_samples=200):\n",
    "    phi = torch.linspace(0, 2 * np.pi, num_samples)\n",
    "    theta = torch.linspace(0, np.pi, num_samples)\n",
    "    # Densely sample phi and theta on a grid\n",
    "    Phi, Theta = torch.meshgrid(phi, theta)\n",
    "    \n",
    "    R = 2\n",
    "    r = 0.5\n",
    "    x = (R + r * torch.sin(Theta)) * torch.cos(Phi)\n",
    "    y = (R + r * torch.sin(Theta)) * torch.sin(Phi)\n",
    "    z = r * torch.cos(Theta)\n",
    "\n",
    "    points = torch.stack((x.flatten(), y.flatten(), z.flatten()), dim=1)\n",
    "    color = (points - points.min()) / (points.max() - points.min())\n",
    "    \n",
    "    return points, color\n",
    "\n",
    "def sample_hourglass_to_pointcloud(num_samples=200):\n",
    "    u = torch.linspace(-1,1, num_samples)\n",
    "    v = torch.linspace(0,2*np.pi, num_samples)\n",
    "    h = 2\n",
    "    R = 1\n",
    "    u,v = torch.meshgrid(u, v)\n",
    "    x = R * torch.abs(u) * torch.cos(v)\n",
    "    y = h * u\n",
    "    z = R * torch.abs(u) * torch.sin(v)\n",
    "    \n",
    "    points = torch.stack((x.flatten(), y.flatten(), z.flatten()), dim=1)\n",
    "    color = (points - points.min()) / (points.max() - points.min())\n",
    "    \n",
    "    return points, color\n",
    "\n",
    "torus_pts, torus_color = sample_torus_to_pointcloud(num_samples=200)\n",
    "render_pointcloud_to_gif(V=torus_pts, rgb=torus_color, gif_path=\"hw1q5p2_torus.gif\", downsample_factor=1)\n",
    "\n",
    "hourglass_pts, hourglass_color = sample_hourglass_to_pointcloud(num_samples=200)\n",
    "render_pointcloud_to_gif(V=hourglass_pts, rgb=hourglass_color, gif_path=\"hw1q5p2_hourglass.gif\", downsample_factor=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1c44e5",
   "metadata": {},
   "source": [
    "## Q5.2 Results: Torus | Hourglass\n",
    "<image src=\"hw1q5p2_torus.gif\" width=\"256\"/>\n",
    "<image src=\"hw1q5p2_hourglass.gif\" width=\"256\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ade39",
   "metadata": {},
   "source": [
    "## Q5.3 Implicit Surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67a63130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_mesh_to_gif(\n",
    "        mesh: pytorch3d.structures.Meshes,\n",
    "        gif_path: Path):\n",
    "    lights = pytorch3d.renderer.PointLights(location=[[0, 0.0, -4.0]], device=device)\n",
    "    renderer = get_mesh_renderer(image_size=image_size, device=device)\n",
    "\n",
    "    image_list = []\n",
    "    for azimuth in tqdm(range(0, 360, 10), desc=\"Rendering mesh...\"): \n",
    "        R, T = pytorch3d.renderer.look_at_view_transform(6, 10, azimuth)\n",
    "        cameras = pytorch3d.renderer.FoVPerspectiveCameras(R=R, T=T, device=device)\n",
    "        rend = renderer(mesh, cameras=cameras, lights=lights)\n",
    "        img = rend.cpu().numpy()[0, ..., :3].clip(0,1)  # (B, H, W, 4) -> (H, W, 3)\n",
    "        img *= 255\n",
    "        img = img.astype('uint8')\n",
    "        image_list.append(img)\n",
    "        \n",
    "    create_gif_from_image_list(image_list, gif_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97f5ac3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering mesh...: 100%|██████████| 36/36 [00:40<00:00,  1.14s/it]\n",
      "Rendering mesh...: 100%|██████████| 36/36 [00:24<00:00,  1.48it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_torus_mesh(voxel_size=64):\n",
    "    min_value = -1.6\n",
    "    max_value = 1.6\n",
    "    R = 1\n",
    "    r = 0.5\n",
    "    X, Y, Z = torch.meshgrid([torch.linspace(min_value, max_value, voxel_size)] * 3)\n",
    "    voxels = (torch.sqrt(X**2 + Y**2) - R)**2 + Z**2 - r**2\n",
    "    vertices, faces = mcubes.marching_cubes(mcubes.smooth(voxels), isovalue=0)\n",
    "    vertices = torch.tensor(vertices).float()\n",
    "    faces = torch.tensor(faces.astype(int))\n",
    "    # Vertex coordinates are indexed by array position, so we need to\n",
    "    # renormalize the coordinate system.\n",
    "    vertices = (vertices / voxel_size) * (max_value - min_value) + min_value\n",
    "    textures = (vertices - vertices.min()) / (vertices.max() - vertices.min())\n",
    "    textures = pytorch3d.renderer.TexturesVertex(vertices.unsqueeze(0))\n",
    "\n",
    "    mesh = pytorch3d.structures.Meshes([vertices], [faces], textures=textures).to(\n",
    "        device\n",
    "    )\n",
    "    return mesh\n",
    "\n",
    "\n",
    "def create_whatever_mesh(voxel_size=64):\n",
    "    min_value = -5\n",
    "    max_value = 5\n",
    "    X, Y, Z = torch.meshgrid([torch.linspace(min_value, max_value, voxel_size)] * 3)\n",
    "    voxels = X**4 + Y**4 + Z**4 + 2 * X**2 * Y**2 + 2 * X**2 * Z**2 + 2 * Y**2 * Z**2 + 8*X*Y*Z - 8 * X**2 - 8 * Y**2 - 8 * Z**2 + 15\n",
    "    vertices, faces = mcubes.marching_cubes(mcubes.smooth(voxels), isovalue=0)\n",
    "    vertices = torch.tensor(vertices).float()\n",
    "    faces = torch.tensor(faces.astype(int))\n",
    "    # Vertex coordinates are indexed by array position, so we need to\n",
    "    # renormalize the coordinate system.\n",
    "    vertices = (vertices / voxel_size) * (max_value - min_value) + min_value\n",
    "    textures = (vertices - vertices.min()) / (vertices.max() - vertices.min())\n",
    "    textures = pytorch3d.renderer.TexturesVertex(vertices.unsqueeze(0))\n",
    "\n",
    "    mesh = pytorch3d.structures.Meshes([vertices], [faces], textures=textures).to(device)\n",
    "    return mesh\n",
    "\n",
    "mesh = create_torus_mesh()\n",
    "render_mesh_to_gif(mesh, \"hw1q5p3_torus.gif\")\n",
    "\n",
    "mesh2 = create_whatever_mesh()\n",
    "render_mesh_to_gif(mesh2, \"hw1q5p3_whatever.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e5613c",
   "metadata": {},
   "source": [
    "## Q5.3 Implicit Surfaces: Torus | Custom Object\n",
    "<image src=\"hw1q5p3_torus.gif\" width=\"256\">\n",
    "<image src=\"hw1q5p3_whatever.gif\" width=\"256\"/>\n",
    "\n",
    "## Discussion of Tradeoffs: Meshes vs. PointCloud\n",
    "\n",
    "In terms of memory usage, point clouds consume significantly more memory to store a high fidelity recording of an object compared to meshes. Meshes can be sampled to recover point cloud information if required, but the reverse process is not as simple, so storing a mesh of an object compared to a dense point cloud will always be advantageous.\n",
    "\n",
    "In terms of rendering speed, GPUs are extremely efficient at rendering meshes, whereas point clouds at high resolution can contain orders of magnitude more points that need to be rendered. Running renders of point clouds in this assignment on the CPU frequently required downsampling of the point cloud for a speedup.\n",
    "\n",
    "In terms of ease of use - especially in deforming, or slicing objects - meshes have defined topologies that make object modification and simulation easier, whereas the points in point clouds have to be individually modified resulting in higher computational loads.\n",
    "\n",
    "In terms of structure, point clouds have an edge due to being able to capture arbitrary points in space, which would allow quick modification of a scene by taking the union or intersection of points to add or remove specific parts of the scene. It is more difficult to arbitrarily manipulate a single mesh to add, subtract, or modify existing features that disrupt the structure.\n",
    "\n",
    "So, while point clouds are less efficient to render and store, it allows for greater flexibility. It is always possible to sample from a mesh to create a point cloud, though the reverse process is more involved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d2fa4a",
   "metadata": {},
   "source": [
    "# Q6: Do Something Fun - Retexturing the cow again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e71137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering cow...: 100%|██████████| 36/36 [00:08<00:00,  4.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get the renderer.\n",
    "renderer = get_mesh_renderer(image_size=image_size)\n",
    "\n",
    "# Get the vertices, faces, and textures.\n",
    "vertices_cow, faces_cow = load_cow_mesh(cow_path)\n",
    "vertices = vertices_cow.unsqueeze(0)  # (N_v, 3) -> (1, N_v, 3)\n",
    "faces = faces_cow.unsqueeze(0)  # (N_f, 3) -> (1, N_f, 3)\n",
    "\n",
    "\n",
    "tail_pt_idx = torch.argmax(vertices[0,:,2])\n",
    "tail_pt = vertices[0,tail_pt_idx,:]\n",
    "rs = torch.linalg.norm(vertices[0,:,:]-tail_pt,dim=1)\n",
    "max_r = torch.max(rs)\n",
    "color1 = torch.tensor([[1,0,0]], dtype=torch.float32)\n",
    "color2 = torch.tensor([[0,1,0]], dtype=torch.float32)\n",
    "alphas = (rs / max_r).unsqueeze(1)\n",
    "textures = (alphas @ color2 + (1 - alphas) @ color1).unsqueeze(0) # (1, N_v, 3)\n",
    "\n",
    "mesh = pytorch3d.structures.Meshes(\n",
    "    verts=vertices,\n",
    "    faces=faces,\n",
    "    textures=pytorch3d.renderer.TexturesVertex(textures),\n",
    ")\n",
    "mesh = mesh.to(device)\n",
    "\n",
    "\n",
    "# Place a point light in front of the cow.\n",
    "lights = pytorch3d.renderer.PointLights(location=[[0, 0, -3]], device=device)\n",
    "\n",
    "images_list = []\n",
    "\n",
    "for i in tqdm(range(0,360,10), desc=\"Rendering cow...\"):\n",
    "\n",
    "    theta = np.radians(i)\n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    R = torch.tensor([[c, 0, s], [0, 1, 0], [-s, 0, c]]).unsqueeze(0)\n",
    "\n",
    "    # Prepare the camera:\n",
    "    cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "        R=R, T=torch.tensor([[0, 0, 3]]), fov=60, device=device\n",
    "    )\n",
    "    \n",
    "    rend = renderer(mesh, cameras=cameras, lights=lights)\n",
    "    img = rend.cpu().numpy()[0, ..., :3]\n",
    "        \n",
    "    img *= 255\n",
    "    img = img.astype('uint8')\n",
    "    images_list.append(img)\n",
    "\n",
    "create_gif_from_image_list(images_list, Path('hw1q6_color.gif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f98b50",
   "metadata": {},
   "source": [
    "For this part, I wanted to explore coloring the cow mesh creatively, based on each vertex's radial distance from a specified point. I found the tail-most point by getting the point with the largest Z-value, computing the distance to that vertex from every other vertex, then coloring vertices based on their distance to that tail point (red = closer, green = farther).\n",
    "\n",
    "## Result\n",
    "<img src=\"hw1q6_color.gif\" width=\"256\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08881792",
   "metadata": {},
   "source": [
    "# Q7: Sampling Points on Meshes\n",
    "Not attempted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e5b51b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "l3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
